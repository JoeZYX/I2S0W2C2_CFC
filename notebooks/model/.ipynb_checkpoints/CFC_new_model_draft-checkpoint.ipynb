{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5785e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1dc1c7",
   "metadata": {},
   "source": [
    "# Cross Channel Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9abea",
   "metadata": {},
   "source": [
    "## attention  interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e542beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv1d(ni: int, no: int, ks: int = 1, stride: int = 1, padding: int = 0, bias: bool = False):\n",
    "#     \"\"\"\n",
    "#     Create and initialize a `nn.Conv1d` layer with spectral normalization.\n",
    "#     \"\"\"\n",
    "#     conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "#     nn.init.kaiming_normal_(conv.weight)\n",
    "#     if bias:\n",
    "#         conv.bias.data.zero_()\n",
    "#     return conv\n",
    "\n",
    "# class SelfAttention_interaction(nn.Module):\n",
    "#     \"\"\"\n",
    "\n",
    "#     \"\"\"\n",
    "#     def __init__(self, n_channels: int, div=1):\n",
    "#         super(SelfAttention_interaction, self).__init__()\n",
    "\n",
    "#         if n_channels > 1:\n",
    "#             self.query = conv1d(n_channels, n_channels//div)\n",
    "#             self.key = conv1d(n_channels, n_channels//div)\n",
    "#         else:\n",
    "#             self.query = conv1d(n_channels, n_channels)\n",
    "#             self.key = conv1d(n_channels, n_channels)\n",
    "#         self.value = conv1d(n_channels, n_channels)\n",
    "#         self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Notation from https://arxiv.org/pdf/1805.08318.pdf\n",
    "#         # 输入尺寸是 batch feature_dim sensor_channel\n",
    "\n",
    "\n",
    "#         f, g, h = self.query(x), self.key(x), self.value(x)\n",
    "        \n",
    "#         beta = F.softmax(torch.bmm(f.permute(0, 2, 1).contiguous(), g), dim=1)\n",
    "        \n",
    "#         o = self.gamma * torch.bmm(h, beta) + x\n",
    "#         # 输出的尺寸是 batch feature_dim sensor_channel 1\n",
    "#         return o.unsqueeze(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3c1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_interaction(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels):\n",
    "        super(SelfAttention_interaction, self).__init__()\n",
    "\n",
    "        self.query = nn.Linear(n_channels, n_channels, bias=False)\n",
    "        self.key = nn.Linear(n_channels, n_channels, bias=False)\n",
    "        self.value = nn.Linear(n_channels, n_channels, bias=False)\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 输入尺寸是 batch  sensor_channel feature_dim\n",
    "        #print(x.shape)\n",
    "\n",
    "        f, g, h = self.query(x), self.key(x), self.value(x)\n",
    "        \n",
    "        beta = F.softmax(torch.bmm(f, g.permute(0, 2, 1).contiguous()), dim=1)\n",
    "\n",
    "        o = self.gamma * torch.bmm(h.permute(0, 2, 1).contiguous(), beta) + x.permute(0, 2, 1).contiguous()\n",
    "        # 输出是 batch  sensor_channel feature_dim 1 \n",
    "        return o.permute(0, 2, 1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21f73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e8783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e133ffcc",
   "metadata": {},
   "source": [
    "## transformer interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49317d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 16, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer_interaction(nn.Module):\n",
    "    def __init__(self, dim, depth=1, heads=4, dim_head=16, mlp_dim=16, dropout = 0.):\n",
    "        super(Transformer_interaction,self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f91bba",
   "metadata": {},
   "source": [
    "## Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b17018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63146617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31462000",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosschannel_interaction = {\"attn\":SelfAttention_interaction,\n",
    "                            \"transformer\": Transformer_interaction,\n",
    "                            \"identity\": Identity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892c8b5",
   "metadata": {},
   "source": [
    "# Cross Channel Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b8f07",
   "metadata": {},
   "source": [
    "## FilterWeighted_Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5818b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterWeighted_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels):\n",
    "        super(FilterWeighted_Aggregation, self).__init__()\n",
    "        self.value_projection = nn.Linear(n_channels, n_channels)\n",
    "        self.value_activation = nn.ReLU() \n",
    "        \n",
    "        self.weight_projection = nn.Linear(n_channels, n_channels)\n",
    "        self.weighs_activation = nn.Tanh() \n",
    "        self.softmatx = nn.Softmax(dim=1)        \n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 输入是 batch  sensor_channel feature_dim\n",
    "\n",
    "\n",
    "        weights = self.weighs_activation(self.weight_projection(x))\n",
    "        weights = self.softmatx(weights)\n",
    "        \n",
    "        values  = self.value_activation(self.value_projection(x))\n",
    "\n",
    "        values  = torch.mul(values, weights)\n",
    "        # 返回是 batch feature_dim\n",
    "        return torch.sum(values,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f14fe",
   "metadata": {},
   "source": [
    "## NaiveWeighted_Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77929495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveWeighted_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(NaiveWeighted_Aggregation, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sm = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 输入是 batch  sensor_channel feature_dim\n",
    "        #   B C F\n",
    "\n",
    "        out = self.fc(x).squeeze(2)\n",
    "\n",
    "        weights_att = self.sm(out).unsqueeze(2)\n",
    "        context = torch.sum(weights_att * x, 1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25e593",
   "metadata": {},
   "source": [
    "## Reshape FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135562bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "\n",
    "    def __init__(self, channel_in, channel_out):\n",
    "        super(FC, self).__init__()\n",
    "        self.fc = nn.Linear(channel_in ,channel_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ded906",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosschannel_aggregation = {\"filter\": FilterWeighted_Aggregation,\n",
    "                            \"naive\" : NaiveWeighted_Aggregation,\n",
    "                            \"FC\" : FC}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dbcb22",
   "metadata": {},
   "source": [
    "# Tempotal Info Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc461a4",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f783fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporal_GRU(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_num):\n",
    "        super(temporal_GRU, self).__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            filter_num,\n",
    "            filter_num,\n",
    "            1,\n",
    "            bidirectional=False,\n",
    "            dropout=0.15,\n",
    "            batch_first = True\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Batch length Filter\n",
    "        outputs, h = self.rnn(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a70224",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945850d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporal_LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_num):\n",
    "        super(temporal_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(filter_num, \n",
    "                            filter_num, \n",
    "                            batch_first =True)\n",
    "    def forward(self, x):\n",
    "        # Batch length Filter\n",
    "        outputs, h = self.lstm(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9adc8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_interaction = {\"gru\": temporal_GRU,\n",
    "                        \"lstm\": temporal_LSTM,\n",
    "                        \"attn\"   :SelfAttention_interaction,\n",
    "                        \"transformer\": Transformer_interaction,\n",
    "                        \"identity\" : Identity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6633970",
   "metadata": {},
   "source": [
    "# Temporal Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a4bf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temmporal_aggregation = {\"filter\": FilterWeighted_Aggregation,\n",
    "                         \"naive\" : NaiveWeighted_Aggregation,\n",
    "                         \"FC\" : FC,\n",
    "                         \"identiry\":Identity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133f4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e74eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20fe6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Light_HAR_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape ,\n",
    "        number_class , \n",
    "\n",
    "        filter_num = 16, # 所有层拥有同样的dim\n",
    "\n",
    "        nb_conv_layers = 4,        \n",
    "        filter_size = 5,\n",
    "        \n",
    "        cross_channel_interaction_type = \"attn\",    # attn  transformer  identity\n",
    "        \n",
    "        cross_channel_aggregation_type = \"filter\",  # filter  naive  FC\n",
    "         \n",
    "        temporal_info_interaction_type = \"gru\",     # gru  lstm  attn  transformer  identity\n",
    "        \n",
    "        temporal_info_aggregation_type = \"FC\",      # naive  filter  FC \n",
    "\n",
    "        dropout = 0.2,\n",
    "        activation = \"ReLU\",\n",
    "\n",
    "    ):\n",
    "        super(Light_HAR_Model, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.cross_channel_interaction_type = cross_channel_interaction_type\n",
    "        self.cross_channel_aggregation_type = cross_channel_aggregation_type\n",
    "        self.temporal_info_interaction_type = temporal_info_interaction_type\n",
    "        self.temporal_info_aggregation_type = temporal_info_aggregation_type\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        PART 1 , ============= Channel wise Feature Extraction =============================        \n",
    "        输入的格式为  Batch, filter_num, length, Sensor_channel        \n",
    "        输出格式为为  Batch, filter_num, downsampling_length, Sensor_channel\n",
    "        \"\"\"\n",
    "\n",
    "        layers_conv = []\n",
    "        for i in range(nb_conv_layers):\n",
    "            if i == 0:\n",
    "                in_channel = input_shape[1]\n",
    "            else:\n",
    "                in_channel = filter_num\n",
    "    \n",
    "            layers_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channel, filter_num, (filter_size, 1),(2,1)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(filter_num),\n",
    "\n",
    "            ))\n",
    "        self.layers_conv = nn.ModuleList(layers_conv)\n",
    "        # 这是给最后时间维度 vectorize的时候用的\n",
    "        downsampling_length = self.get_the_shape(input_shape)        \n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        PART2 , ================ Cross Channel interaction  =================================\n",
    "        这里可供选择的  attn   transformer  itentity\n",
    "        输出格式为  Batch, filter_num, downsampling_length, Sensor_channel\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.channel_interaction = crosschannel_interaction[cross_channel_interaction_type](filter_num)\n",
    "        # 这里还是 B F C L  需要permute++++++++++++++\n",
    "\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        PART3 , =============== Cross Channel Fusion  ====================================\n",
    "        这里可供选择的  filter   naive  FC\n",
    "\n",
    "        输出格式为  Batch, downsampling_length, filter_num\n",
    "        \"\"\"\n",
    "        if cross_channel_aggregation_type == \"FC\":\n",
    "            # 这里需要reshape为 B L C*F++++++++++++++\n",
    "            self.channel_fusion = crosschannel_aggregation[cross_channel_aggregation_type](input_shape[3]*filter_num,filter_num)\n",
    "\n",
    "        else:\n",
    "            # 这里需要沿着时间轴走\n",
    "            self.channel_fusion = crosschannel_aggregation[cross_channel_aggregation_type](filter_num)\n",
    "            # --> B F L\n",
    "            # 需要reshape++++++++++++++++++++++++++++++\n",
    "\n",
    "            \n",
    "        # BLF\n",
    "        self.activation = nn.ReLU() \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        PART4  , ============= Temporal information Extraction =========================\n",
    "        这里可供选择的  gru lstm attn transformer   identity\n",
    "\n",
    "        输出格式为  Batch, downsampling_length, filter_num\n",
    "        \"\"\"\n",
    "        \n",
    "        # ++++++++++++ 这里需要讨论\n",
    "        self.temporal_interaction = temporal_interaction[temporal_info_interaction_type](filter_num)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        PART 5 , =================== Temporal information Aggregation ================\n",
    "\n",
    "\n",
    "        输出格式为  Batch, downsampling_length, filter_num\n",
    "        \"\"\"        \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if temporal_info_aggregation_type == \"FC\":\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.temporal_fusion = temmporal_aggregation[temporal_info_aggregation_type](downsampling_length*filter_num,filter_num)\n",
    "        else:\n",
    "            self.temporal_fusion = temmporal_aggregation[temporal_info_aggregation_type](filter_num)\n",
    "            \n",
    "        #--> B F\n",
    "\n",
    "        # PART 6 , ==================== Prediction ==============================\n",
    "        self.prediction = nn.Linear(filter_num ,number_class)\n",
    "\n",
    "    def get_the_shape(self, input_shape):\n",
    "        x = torch.rand(input_shape)\n",
    "\n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)    \n",
    "\n",
    "        return x.shape[2]\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B F L C   \n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = x.permute(0,3,2,1) \n",
    "        # ------->  B x C x L* x F*       \n",
    "\n",
    "\n",
    "\n",
    "        \"\"\" =============== cross channel interaction ===============\"\"\"\n",
    "        x = torch.cat(\n",
    "            [self.channel_interaction(x[:, :, t, :]).unsqueeze(3) for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # ------->  B x C x F* x L* \n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        \"\"\"=============== cross channel fusion ===============\"\"\"\n",
    "        \n",
    "        if self.cross_channel_aggregation_type == \"FC\":\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "            x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "            x = self.activation(self.channel_fusion(x)) # B L C\n",
    "        else:\n",
    "            x = torch.cat(\n",
    "                [self.channel_fusion(x[:, :, :, t]).unsqueeze(2) for t in range(x.shape[3])],\n",
    "                dim=-1,\n",
    "            )\n",
    "            x = x.permute(0,2,1)\n",
    "            x = self.activation(x)\n",
    "        # ------->  B x L* x F*\n",
    "            \n",
    "            \n",
    "        \"\"\"cross temporal interaction \"\"\"\n",
    "        x = self.temporal_interaction(x)\n",
    "\n",
    "\n",
    "        \n",
    "        \"\"\"cross temporal fusion \"\"\"\n",
    "        if self.temporal_info_aggregation_type == \"FC\":\n",
    "            x = self.flatten(x)\n",
    "            x = self.activation(self.temporal_fusion(x)) # B L C\n",
    "        else:\n",
    "            x = self.temporal_fusion(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        y = self.prediction(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a3d5426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1937,  0.0957,  0.1372,  0.2366, -0.2333, -0.1543]],\n",
       "       dtype=torch.float64, grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch         = 1\n",
    "number_filter = 12\n",
    "length        = 128\n",
    "channel       = 6\n",
    "\n",
    "\n",
    "model = Light_HAR_Model((batch, number_filter, length, channel),\n",
    "                        \n",
    "                        filter_num = 16, # 所有层拥有同样的dim\n",
    "\n",
    "                        nb_conv_layers = 4,        \n",
    "                        filter_size = 5,\n",
    "\n",
    "                        cross_channel_interaction_type = \"attn\",    # attn  transformer  identity\n",
    "\n",
    "                        cross_channel_aggregation_type = \"FC\",  # filter  naive  FC\n",
    "\n",
    "                        temporal_info_interaction_type = \"lstm\",     # gru  lstm  attn  transformer  identity\n",
    "\n",
    "                        temporal_info_aggregation_type = \"FC\",      # naive  filter  FC \n",
    "                        \n",
    "                        number_class=6).double()\n",
    "input = torch.rand(batch,number_filter ,length, channel).double()\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "866a0ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn   filter   gru   naive\n",
      "29416\n",
      "attn   filter   gru   filter\n",
      "31495\n",
      "attn   filter   gru   FC\n",
      "34535\n",
      "attn   filter   lstm   naive\n",
      "31528\n",
      "attn   filter   lstm   filter\n",
      "33607\n",
      "attn   filter   lstm   FC\n",
      "36647\n",
      "attn   filter   attn   naive\n",
      "26153\n",
      "attn   filter   attn   filter\n",
      "28232\n",
      "attn   filter   attn   FC\n",
      "31272\n",
      "attn   filter   transformer   naive\n",
      "32504\n",
      "attn   filter   transformer   filter\n",
      "34583\n",
      "attn   filter   transformer   FC\n",
      "37623\n",
      "attn   filter   identity   naive\n",
      "23080\n",
      "attn   filter   identity   filter\n",
      "25159\n",
      "attn   filter   identity   FC\n",
      "28199\n",
      "attn   naive   gru   naive\n",
      "27337\n",
      "attn   naive   gru   filter\n",
      "29416\n",
      "attn   naive   gru   FC\n",
      "32456\n",
      "attn   naive   lstm   naive\n",
      "29449\n",
      "attn   naive   lstm   filter\n",
      "31528\n",
      "attn   naive   lstm   FC\n",
      "34568\n",
      "attn   naive   attn   naive\n",
      "24074\n",
      "attn   naive   attn   filter\n",
      "26153\n",
      "attn   naive   attn   FC\n",
      "29193\n",
      "attn   naive   transformer   naive\n",
      "30425\n",
      "attn   naive   transformer   filter\n",
      "32504\n",
      "attn   naive   transformer   FC\n",
      "35544\n",
      "attn   naive   identity   naive\n",
      "21001\n",
      "attn   naive   identity   filter\n",
      "23080\n",
      "attn   naive   identity   FC\n",
      "26120\n",
      "attn   FC   gru   naive\n",
      "106184\n",
      "attn   FC   gru   filter\n",
      "108263\n",
      "attn   FC   gru   FC\n",
      "111303\n",
      "attn   FC   lstm   naive\n",
      "108296\n",
      "attn   FC   lstm   filter\n",
      "110375\n",
      "attn   FC   lstm   FC\n",
      "113415\n",
      "attn   FC   attn   naive\n",
      "102921\n",
      "attn   FC   attn   filter\n",
      "105000\n",
      "attn   FC   attn   FC\n",
      "108040\n",
      "attn   FC   transformer   naive\n",
      "109272\n",
      "attn   FC   transformer   filter\n",
      "111351\n",
      "attn   FC   transformer   FC\n",
      "114391\n",
      "attn   FC   identity   naive\n",
      "99848\n",
      "attn   FC   identity   filter\n",
      "101927\n",
      "attn   FC   identity   FC\n",
      "104967\n",
      "transformer   filter   gru   naive\n",
      "35767\n",
      "transformer   filter   gru   filter\n",
      "37846\n",
      "transformer   filter   gru   FC\n",
      "40886\n",
      "transformer   filter   lstm   naive\n",
      "37879\n",
      "transformer   filter   lstm   filter\n",
      "39958\n",
      "transformer   filter   lstm   FC\n",
      "42998\n",
      "transformer   filter   attn   naive\n",
      "32504\n",
      "transformer   filter   attn   filter\n",
      "34583\n",
      "transformer   filter   attn   FC\n",
      "37623\n",
      "transformer   filter   transformer   naive\n",
      "38855\n",
      "transformer   filter   transformer   filter\n",
      "40934\n",
      "transformer   filter   transformer   FC\n",
      "43974\n",
      "transformer   filter   identity   naive\n",
      "29431\n",
      "transformer   filter   identity   filter\n",
      "31510\n",
      "transformer   filter   identity   FC\n",
      "34550\n",
      "transformer   naive   gru   naive\n",
      "33688\n",
      "transformer   naive   gru   filter\n",
      "35767\n",
      "transformer   naive   gru   FC\n",
      "38807\n",
      "transformer   naive   lstm   naive\n",
      "35800\n",
      "transformer   naive   lstm   filter\n",
      "37879\n",
      "transformer   naive   lstm   FC\n",
      "40919\n",
      "transformer   naive   attn   naive\n",
      "30425\n",
      "transformer   naive   attn   filter\n",
      "32504\n",
      "transformer   naive   attn   FC\n",
      "35544\n",
      "transformer   naive   transformer   naive\n",
      "36776\n",
      "transformer   naive   transformer   filter\n",
      "38855\n",
      "transformer   naive   transformer   FC\n",
      "41895\n",
      "transformer   naive   identity   naive\n",
      "27352\n",
      "transformer   naive   identity   filter\n",
      "29431\n",
      "transformer   naive   identity   FC\n",
      "32471\n",
      "transformer   FC   gru   naive\n",
      "112535\n",
      "transformer   FC   gru   filter\n",
      "114614\n",
      "transformer   FC   gru   FC\n",
      "117654\n",
      "transformer   FC   lstm   naive\n",
      "114647\n",
      "transformer   FC   lstm   filter\n",
      "116726\n",
      "transformer   FC   lstm   FC\n",
      "119766\n",
      "transformer   FC   attn   naive\n",
      "109272\n",
      "transformer   FC   attn   filter\n",
      "111351\n",
      "transformer   FC   attn   FC\n",
      "114391\n",
      "transformer   FC   transformer   naive\n",
      "115623\n",
      "transformer   FC   transformer   filter\n",
      "117702\n",
      "transformer   FC   transformer   FC\n",
      "120742\n",
      "transformer   FC   identity   naive\n",
      "106199\n",
      "transformer   FC   identity   filter\n",
      "108278\n",
      "transformer   FC   identity   FC\n",
      "111318\n",
      "identity   filter   gru   naive\n",
      "26343\n",
      "identity   filter   gru   filter\n",
      "28422\n",
      "identity   filter   gru   FC\n",
      "31462\n",
      "identity   filter   lstm   naive\n",
      "28455\n",
      "identity   filter   lstm   filter\n",
      "30534\n",
      "identity   filter   lstm   FC\n",
      "33574\n",
      "identity   filter   attn   naive\n",
      "23080\n",
      "identity   filter   attn   filter\n",
      "25159\n",
      "identity   filter   attn   FC\n",
      "28199\n",
      "identity   filter   transformer   naive\n",
      "29431\n",
      "identity   filter   transformer   filter\n",
      "31510\n",
      "identity   filter   transformer   FC\n",
      "34550\n",
      "identity   filter   identity   naive\n",
      "20007\n",
      "identity   filter   identity   filter\n",
      "22086\n",
      "identity   filter   identity   FC\n",
      "25126\n",
      "identity   naive   gru   naive\n",
      "24264\n",
      "identity   naive   gru   filter\n",
      "26343\n",
      "identity   naive   gru   FC\n",
      "29383\n",
      "identity   naive   lstm   naive\n",
      "26376\n",
      "identity   naive   lstm   filter\n",
      "28455\n",
      "identity   naive   lstm   FC\n",
      "31495\n",
      "identity   naive   attn   naive\n",
      "21001\n",
      "identity   naive   attn   filter\n",
      "23080\n",
      "identity   naive   attn   FC\n",
      "26120\n",
      "identity   naive   transformer   naive\n",
      "27352\n",
      "identity   naive   transformer   filter\n",
      "29431\n",
      "identity   naive   transformer   FC\n",
      "32471\n",
      "identity   naive   identity   naive\n",
      "17928\n",
      "identity   naive   identity   filter\n",
      "20007\n",
      "identity   naive   identity   FC\n",
      "23047\n",
      "identity   FC   gru   naive\n",
      "103111\n",
      "identity   FC   gru   filter\n",
      "105190\n",
      "identity   FC   gru   FC\n",
      "108230\n",
      "identity   FC   lstm   naive\n",
      "105223\n",
      "identity   FC   lstm   filter\n",
      "107302\n",
      "identity   FC   lstm   FC\n",
      "110342\n",
      "identity   FC   attn   naive\n",
      "99848\n",
      "identity   FC   attn   filter\n",
      "101927\n",
      "identity   FC   attn   FC\n",
      "104967\n",
      "identity   FC   transformer   naive\n",
      "106199\n",
      "identity   FC   transformer   filter\n",
      "108278\n",
      "identity   FC   transformer   FC\n",
      "111318\n",
      "identity   FC   identity   naive\n",
      "96775\n",
      "identity   FC   identity   filter\n",
      "98854\n",
      "identity   FC   identity   FC\n",
      "101894\n"
     ]
    }
   ],
   "source": [
    "batch         = 1\n",
    "number_filter = 12\n",
    "length        = 128\n",
    "channel       = 77\n",
    "\n",
    "for a in [\"attn\" , \"transformer\" , \"identity\"]:\n",
    "    for b in [\"filter\",  \"naive\",  \"FC\"]:\n",
    "        for c in [\"gru\",  \"lstm\",  \"attn\",  \"transformer\" , \"identity\"]:\n",
    "            for d in [\"naive\",  \"filter\",  \"FC\" ]:\n",
    "                \n",
    "                print(a,\" \", b ,\" \",c, \" \",d)\n",
    "    \n",
    "                model = Light_HAR_Model((batch, number_filter, length, channel),\n",
    "\n",
    "                                        filter_num = 32, # 所有层拥有同样的dim\n",
    "\n",
    "                                        nb_conv_layers = 4,        \n",
    "                                        filter_size = 5,\n",
    "\n",
    "                                        cross_channel_interaction_type = a,    # attn  transformer  identity\n",
    "\n",
    "                                        cross_channel_aggregation_type = b,  # filter  naive  FC\n",
    "\n",
    "                                        temporal_info_interaction_type = c,     # gru  lstm  attn  transformer  identity\n",
    "\n",
    "                                        temporal_info_aggregation_type = d,      # naive  filter  FC \n",
    "\n",
    "                                        number_class=6).double()\n",
    "        \n",
    "                input = torch.rand(batch,number_filter ,length, channel).double()\n",
    "                out = model(input)\n",
    "                print(np.sum([para.numel() for para in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77813e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df843d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c9aab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "372d9aea",
   "metadata": {},
   "source": [
    "# import numpy as np\n",
    "np.sum([para.numel() for para in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7fa39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfa635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv1d(ni: int, no: int, ks: int = 1, stride: int = 1, padding: int = 0, bias: bool = False):\n",
    "    \"\"\"\n",
    "    Create and initialize a `nn.Conv1d` layer with spectral normalization.\n",
    "    \"\"\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias:\n",
    "        conv.bias.data.zero_()\n",
    "    # return spectral_norm(conv)\n",
    "    return conv\n",
    "\n",
    "class SelfAttention_crosschannel_interaction(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels: int, div):\n",
    "        super(SelfAttention_crosschannel_interaction, self).__init__()\n",
    "\n",
    "        if n_channels > 1:\n",
    "            self.query = conv1d(n_channels, n_channels//div)\n",
    "            self.key = conv1d(n_channels, n_channels//div)\n",
    "        else:\n",
    "            self.query = conv1d(n_channels, n_channels)\n",
    "            self.key = conv1d(n_channels, n_channels)\n",
    "        self.value = conv1d(n_channels, n_channels)\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Notation from https://arxiv.org/pdf/1805.08318.pdf\n",
    "        size = x.size()\n",
    "        #print(\"size+\",size)\n",
    "        x = x.view(*size[:2], -1)\n",
    "        #print(\"size-\",x.size())\n",
    "        f, g, h = self.query(x), self.key(x), self.value(x)\n",
    "        beta = F.softmax(torch.bmm(f.permute(0, 2, 1).contiguous(), g), dim=1)\n",
    "        o = self.gamma * torch.bmm(h, beta) + x\n",
    "        return o.view(*size).contiguous()\n",
    "\n",
    "    \n",
    "class SelfAttention_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels):\n",
    "        super(SelfAttention_Aggregation, self).__init__()\n",
    "        self.value_projection = nn.Linear(n_channels, n_channels)\n",
    "        self.weight_projection = nn.Linear(n_channels, n_channels)\n",
    "        self.softmatx = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        weights = self.weight_projection(x)\n",
    "        weights = self.softmatx(weights)\n",
    "        values  = self.value_projection(x)\n",
    "        values  = torch.mul(values, weights)\n",
    "        return torch.sum(values,dim=1).unsqueeze(2)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sm = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x).squeeze(2)\n",
    "        weights_att = self.sm(out).unsqueeze(2)\n",
    "        context = torch.sum(weights_att * x, 0)\n",
    "        return context\n",
    "    \n",
    "class CFC_V4_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape ,\n",
    "        number_class , \n",
    "        filter_num = 16,\n",
    "        filter_size = 5,\n",
    "        nb_conv_layers = 4,\n",
    "        dropout = 0.2,\n",
    "        hidden_dim = 16,\n",
    "        activation = \"ReLU\",\n",
    "        sa_div= 1,\n",
    "    ):\n",
    "        super(CFC_V4_Model, self).__init__()\n",
    "        \n",
    "        # PART 1 , ============= Channel wise Feature Extraction =============================\n",
    "        \n",
    "        layers_conv = []\n",
    "        for i in range(nb_conv_layers):\n",
    "            if i == 0:\n",
    "                in_channel = input_shape[1]\n",
    "            else:\n",
    "                in_channel = filter_num\n",
    "    \n",
    "            layers_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channel, filter_num, (filter_size, 1),(2,1)),#(2,1)\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(filter_num),\n",
    "\n",
    "            ))\n",
    "        \n",
    "        self.layers_conv = nn.ModuleList(layers_conv)\n",
    "\n",
    "        # PART2 , ================ Cross Channel interaction  =================================\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.channel_interaction = SelfAttention_crosschannel_interaction(filter_num, sa_div)\n",
    "        \n",
    "\n",
    "        # PART3 , =============== Cross Channel Fusion  ====================================\n",
    "\n",
    "        \n",
    "        self.channel_fusion = SelfAttention_Aggregation(filter_num)\n",
    "    \n",
    "        # PART4  , ============= Temporal information Extraction =========================\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            filter_num,\n",
    "            hidden_dim,\n",
    "            2,\n",
    "            bidirectional=False,\n",
    "            dropout=0.15,\n",
    "        )\n",
    "        # PART 5 , =================== Temporal information Aggregation ================\n",
    "        self.temporal_fusion = TemporalAttention(hidden_dim)\n",
    "\n",
    "        # PART 6 , ==================== Prediction ==============================\n",
    "        self.prediction = nn.Linear(hidden_dim ,number_class)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B F L C   F==1 or F==Nds+1\n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)      \n",
    "\n",
    "        # apply self-attention on each temporal dimension (along sensor and feature dimensions)\n",
    "\n",
    "        x = torch.cat(\n",
    "            [self.channel_interaction(torch.unsqueeze(x[:, :, t, :], dim=3)) for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # x B F C L\n",
    "\n",
    "        x = x.permute(0,2,1,3)\n",
    "        # refined B C F L\n",
    "\n",
    "        x = torch.cat(\n",
    "            [self.channel_fusion(x[:, :, :,t]) for t in range(x.shape[3])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # refined B F L\n",
    "        \n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(2,0,1)\n",
    "        x, h = self.rnn(x) # L B  F\n",
    "\n",
    "        x = self.temporal_fusion(x)\n",
    "        y = self.prediction(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e1497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386c449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2dcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6f1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065eecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "335f6708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1149, 0.1393],\n",
       "         [0.1138, 0.7365],\n",
       "         [0.4094, 0.1016],\n",
       "         [0.3620, 0.0226]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "m = nn.Softmax(dim=1)\n",
    "input = torch.randn(1, 4, 2)\n",
    "output = m(input)\n",
    "#torch.sum(output,dim=1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b367a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1948, -0.1908]], dtype=torch.float64, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels):\n",
    "        super(SelfAttention_Aggregation, self).__init__()\n",
    "        self.value_projection = nn.Linear(n_channels, n_channels)\n",
    "        self.weight_projection = nn.Linear(n_channels, n_channels)\n",
    "        self.softmatx = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        weights = self.weight_projection(x)\n",
    "        weights = self.softmatx(weights)\n",
    "        values  = self.value_projection(x)\n",
    "        values  = torch.mul(values, weights)\n",
    "        return torch.sum(values,dim=1)\n",
    "a = SelfAttention_Aggregation(2).double()\n",
    "input = torch.rand(1,10,2).double()\n",
    "a(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c489761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157aef71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb920274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv1d(ni: int, no: int, ks: int = 1, stride: int = 1, padding: int = 0, bias: bool = False):\n",
    "    \"\"\"\n",
    "    Create and initialize a `nn.Conv1d` layer with spectral normalization.\n",
    "    \"\"\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias:\n",
    "        conv.bias.data.zero_()\n",
    "    # return spectral_norm(conv)\n",
    "    return conv\n",
    "\n",
    "class SelfAttention_crosschannel_interaction(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels: int, div):\n",
    "        super(SelfAttention_crosschannel_interaction, self).__init__()\n",
    "\n",
    "        if n_channels > 1:\n",
    "            self.query = conv1d(n_channels, n_channels//div)\n",
    "            self.key = conv1d(n_channels, n_channels//div)\n",
    "        else:\n",
    "            self.query = conv1d(n_channels, n_channels)\n",
    "            self.key = conv1d(n_channels, n_channels)\n",
    "        self.value = conv1d(n_channels, n_channels)\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Notation from https://arxiv.org/pdf/1805.08318.pdf\n",
    "        size = x.size()\n",
    "        #print(\"size+\",size)\n",
    "        x = x.view(*size[:2], -1)\n",
    "        #print(\"size-\",x.size())\n",
    "        f, g, h = self.query(x), self.key(x), self.value(x)\n",
    "        beta = F.softmax(torch.bmm(f.permute(0, 2, 1).contiguous(), g), dim=1)\n",
    "        o = self.gamma * torch.bmm(h, beta) + x\n",
    "        return o.view(*size).contiguous()\n",
    "\n",
    "    \n",
    "class SelfAttention_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels):\n",
    "        super(SelfAttention_Aggregation, self).__init__()\n",
    "        self.value_projection = nn.Linear(n_channels, n_channels)\n",
    "        self.weight_projection = nn.Linear(n_channels, n_channels)\n",
    "        self.softmatx = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        weights = self.weight_projection(x)\n",
    "        weights = self.softmatx(weights)\n",
    "        values  = self.value_projection(x)\n",
    "        values  = torch.mul(values, weights)\n",
    "        return torch.sum(values,dim=1).unsqueeze(2)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sm = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x).squeeze(2)\n",
    "        weights_att = self.sm(out).unsqueeze(2)\n",
    "        context = torch.sum(weights_att * x, 0)\n",
    "        return context\n",
    "    \n",
    "class CFC_V4_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape ,\n",
    "        number_class , \n",
    "        filter_num = 16,\n",
    "        filter_size = 5,\n",
    "        nb_conv_layers = 4,\n",
    "        dropout = 0.2,\n",
    "        hidden_dim = 16,\n",
    "        activation = \"ReLU\",\n",
    "        sa_div= 1,\n",
    "    ):\n",
    "        super(CFC_V4_Model, self).__init__()\n",
    "        \n",
    "        # PART 1 , ============= Channel wise Feature Extraction =============================\n",
    "        \n",
    "        layers_conv = []\n",
    "        for i in range(nb_conv_layers):\n",
    "            if i == 0:\n",
    "                in_channel = input_shape[1]\n",
    "            else:\n",
    "                in_channel = filter_num\n",
    "    \n",
    "            layers_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channel, filter_num, (filter_size, 1),(2,1)),#(2,1)\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(filter_num),\n",
    "\n",
    "            ))\n",
    "        \n",
    "        self.layers_conv = nn.ModuleList(layers_conv)\n",
    "\n",
    "        # PART2 , ================ Cross Channel interaction  =================================\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.channel_interaction = SelfAttention_crosschannel_interaction(filter_num, sa_div)\n",
    "        \n",
    "\n",
    "        # PART3 , =============== Cross Channel Fusion  ====================================\n",
    "\n",
    "        \n",
    "        self.channel_fusion = SelfAttention_Aggregation(filter_num)\n",
    "    \n",
    "        # PART4  , ============= Temporal information Extraction =========================\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            filter_num,\n",
    "            hidden_dim,\n",
    "            2,\n",
    "            bidirectional=False,\n",
    "            dropout=0.15,\n",
    "        )\n",
    "        # PART 5 , =================== Temporal information Aggregation ================\n",
    "        self.temporal_fusion = TemporalAttention(hidden_dim)\n",
    "\n",
    "        # PART 6 , ==================== Prediction ==============================\n",
    "        self.prediction = nn.Linear(hidden_dim ,number_class)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B F L C   F==1 or F==Nds+1\n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)      \n",
    "\n",
    "        # apply self-attention on each temporal dimension (along sensor and feature dimensions)\n",
    "\n",
    "        x = torch.cat(\n",
    "            [self.channel_interaction(torch.unsqueeze(x[:, :, t, :], dim=3)) for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # x B F C L\n",
    "\n",
    "        x = x.permute(0,2,1,3)\n",
    "        # refined B C F L\n",
    "\n",
    "        x = torch.cat(\n",
    "            [self.channel_fusion(x[:, :, :,t]) for t in range(x.shape[3])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # refined B F L\n",
    "        \n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(2,0,1)\n",
    "        x, h = self.rnn(x) # L B  F\n",
    "\n",
    "        x = self.temporal_fusion(x)\n",
    "        y = self.prediction(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3810b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 16])\n",
      "torch.Size([2, 6, 16])\n",
      "torch.Size([2, 6, 16])\n",
      "torch.Size([2, 6, 16])\n",
      "torch.Size([2, 6, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0572,  0.1141, -0.0173,  0.2453,  0.0325, -0.1014],\n",
       "        [-0.0833,  0.1265, -0.0186,  0.2403,  0.0312, -0.1002]],\n",
       "       dtype=torch.float64, grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batch = 2\n",
    "F_in  = 50\n",
    "Leng  = 128\n",
    "C_in = 6\n",
    "number_class = 6\n",
    "model = CFC_V4_Model((Batch,F_in,Leng, C_in),number_class,filter_num = 16).double()\n",
    "input = torch.rand(Batch,F_in,Leng, C_in).double()\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4ba963a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12728"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum([para.numel() for para in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a9502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d40b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5bc16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e39d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1f2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7ec95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a80992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aba19ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv1d(ni: int, no: int, ks: int = 1, stride: int = 1, padding: int = 0, bias: bool = False):\n",
    "    \"\"\"\n",
    "    Create and initialize a `nn.Conv1d` layer with spectral normalization.\n",
    "    \"\"\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias:\n",
    "        conv.bias.data.zero_()\n",
    "    # return spectral_norm(conv)\n",
    "    return conv\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels: int, div):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        if n_channels > 1:\n",
    "            self.query = conv1d(n_channels, n_channels//div)\n",
    "            self.key = conv1d(n_channels, n_channels//div)\n",
    "        else:\n",
    "            self.query = conv1d(n_channels, n_channels)\n",
    "            self.key = conv1d(n_channels, n_channels)\n",
    "        self.value = conv1d(n_channels, n_channels)\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Notation from https://arxiv.org/pdf/1805.08318.pdf\n",
    "        size = x.size()\n",
    "        #print(\"size+\",size)\n",
    "        x = x.view(*size[:2], -1)\n",
    "        #print(\"size-\",x.size())\n",
    "        f, g, h = self.query(x), self.key(x), self.value(x)\n",
    "        beta = F.softmax(torch.bmm(f.permute(0, 2, 1).contiguous(), g), dim=1)\n",
    "        o = self.gamma * torch.bmm(h, beta) + x\n",
    "        return o.view(*size).contiguous()\n",
    "    \n",
    "class CFC_V3_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape ,\n",
    "        number_class , \n",
    "        filter_num = 16,\n",
    "        hidden_dim = 16,\n",
    "        filter_size = 5,\n",
    "        nb_conv_layers = 4,\n",
    "        dropout = 0.2,\n",
    "        activation = \"ReLU\",\n",
    "        sa_div= 1,\n",
    "    ):\n",
    "        super(CFC_V3_Model, self).__init__()\n",
    "        \n",
    "        # PART 1 , Channel wise Feature Extraction\n",
    "        \n",
    "        layers_conv = []\n",
    "        for i in range(nb_conv_layers):\n",
    "        \n",
    "            if i == 0:\n",
    "                in_channel = input_shape[1]\n",
    "            else:\n",
    "                in_channel = filter_num\n",
    "    \n",
    "            layers_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channel, filter_num, (filter_size, 1),(2,1)),#(2,1)\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(filter_num),\n",
    "\n",
    "            ))\n",
    "        \n",
    "        self.layers_conv = nn.ModuleList(layers_conv)\n",
    "\n",
    "        # PART2 , Cross Channel Fusion through Attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sa = SelfAttention(filter_num, sa_div)\n",
    "        \n",
    "\n",
    "\n",
    "        # PART 3 , Prediction \n",
    "        \n",
    "        self.activation = nn.ReLU() \n",
    "        self.fc1 = nn.Linear(input_shape[3]*filter_num ,filter_num)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            filter_num,\n",
    "            hidden_dim,\n",
    "            2,\n",
    "            bidirectional=False,\n",
    "            dropout=0.15,\n",
    "        )\n",
    "\n",
    "        self.prediction = nn.Linear(hidden_dim ,number_class)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B ? L C\n",
    "        # x = x.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)      \n",
    "\n",
    "\n",
    "        batch, filter, length, channel = x.shape\n",
    "\n",
    "\n",
    "        # apply self-attention on each temporal dimension (along sensor and feature dimensions)\n",
    "        refined = torch.cat(\n",
    "            [self.sa(torch.unsqueeze(x[:, :, t, :], dim=3)) for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "\n",
    "       # print(refined.shape)\n",
    "\n",
    "        x = refined.permute(0, 3, 1, 2)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.activation(self.fc1(x)) # B L F\n",
    "        x = x.permute(1,0,2)\n",
    "\n",
    "        outputs, h = self.rnn(x) # L B  F\n",
    "        x = outputs[-1, :, :]\n",
    "        y = self.prediction(x)    \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645b3c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2748, -0.1322, -0.0927,  0.0486,  0.0153,  0.2842],\n",
       "        [ 0.2229, -0.1758, -0.1539,  0.0666,  0.0259,  0.3083]],\n",
       "       dtype=torch.float64, grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batch = 2\n",
    "F_in  = 50\n",
    "Leng  = 128\n",
    "C_in = 6\n",
    "number_class = 6\n",
    "model = CFC_V3_Model((Batch,F_in,Leng, C_in),number_class,filter_num = 16).double()\n",
    "input = torch.rand(Batch,F_in,Leng, C_in).double()\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5150b4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13719"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum([para.numel() for para in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377bce34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b311b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ba6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "551ad7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# -------------- Transformer Encoder -----------\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 16, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class AggregationAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AggregationAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sm = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x).squeeze(2)\n",
    "        weights_att = self.sm(out).unsqueeze(2)\n",
    "        context = torch.sum(weights_att * x, 0)\n",
    "        return context\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth=1, heads=4, dim_head=16, mlp_dim=16, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "        self.aggregation = AggregationAttention(dim)\n",
    "    def forward(self, x):\n",
    "        # B F C 1\n",
    "        size = x.size()\n",
    "        # --> B C F\n",
    "        x = x.view(*size[:2], -1).permute(0,2,1)\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        # --- B C F \n",
    "        #x = self.aggregation(x.permute(1,0,2))\n",
    "        return x.permute(0,2,1).unsqueeze(3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class CFC_V2_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape ,\n",
    "        number_class , \n",
    "        filter_num = 32,\n",
    "        filter_size = 5,\n",
    "        nb_conv_layers = 4,\n",
    "        dropout = 0.2,\n",
    "        hidden_dim = 32,\n",
    "        activation = \"ReLU\",\n",
    "        sa_div= 1,\n",
    "    ):\n",
    "        super(CFC_V2_Model, self).__init__()\n",
    "        \n",
    "        # PART 1 , Channel wise Feature Extraction\n",
    "        \n",
    "        layers_conv = []\n",
    "        for i in range(nb_conv_layers):\n",
    "        \n",
    "            if i == 0:\n",
    "                in_channel = input_shape[1]\n",
    "            else:\n",
    "                in_channel = filter_num\n",
    "            if i%2==0:\n",
    "                stride = 2\n",
    "            else:\n",
    "                stride = 2\n",
    "            layers_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channel, filter_num, (filter_size, 1),(stride,1)),#(2,1)\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(filter_num),\n",
    "            ))\n",
    "        \n",
    "        self.layers_conv = nn.ModuleList(layers_conv)\n",
    "\n",
    "        # PART2 , Cross Channel Fusion through Attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #self.sa = SelfAttention(filter_num, sa_div)\n",
    "        self.channel_aggregation = Transformer(dim=filter_num, depth=1, heads=4, dim_head=16, mlp_dim=filter_num, dropout = 0.)\n",
    "\n",
    "\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            filter_num*input_shape[3],\n",
    "            hidden_dim,\n",
    "            2,\n",
    "            bidirectional=False,\n",
    "            dropout=0.15,\n",
    "        )\n",
    "        \n",
    "        #self.temporal_aggregation = AggregationAttention(hidden_dim)\n",
    "\n",
    "#         # PART 3 , Prediction \n",
    "        self.prediction = nn.Linear(hidden_dim, number_class)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B F , L C  F =1 or F =filter * scale + 1\n",
    "\n",
    "        \n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)      \n",
    "        # B filter_num  L  C\n",
    "\n",
    "        batch, filter, length, channel = x.shape\n",
    "        # 每次进去的都是 B filter_num C 1\n",
    "        refined = torch.cat(\n",
    "            [self.channel_aggregation(torch.unsqueeze(x[:, :, t, :], dim=3)) for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # B F C L\n",
    "\n",
    "        print(refined.shape)\n",
    "        x = refined.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        print(x.shape)\n",
    "        # B L  F*C\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        print(x.shape)\n",
    "        outputs, h = self.rnn(x)\n",
    "\n",
    "        # L B F\n",
    "        x = outputs[-1, :, :]\n",
    "        print(outputs.shape)\n",
    "        # B  F \n",
    "\n",
    "        y = self.prediction(x)    \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6db6bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 31, 6, 13])\n",
      "torch.Size([2, 13, 186])\n",
      "torch.Size([13, 2, 186])\n",
      "torch.Size([13, 2, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0557, -0.4428, -0.1118, -0.0205,  0.1939, -0.1840],\n",
       "        [ 0.1468,  0.0677, -0.0788,  0.1941,  0.1573,  0.0115]],\n",
       "       dtype=torch.float64, grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batch = 2\n",
    "F_in  = 100\n",
    "Leng  = 256\n",
    "C_in = 6\n",
    "number_class = 6\n",
    "model = CFC_V2_Model((Batch,F_in,Leng, C_in),number_class,filter_num = 31).double()\n",
    "input = torch.rand(Batch,F_in,Leng, C_in).double()\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b28ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9d261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a12657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79850d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bf3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Transformer Encoder -----------\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 16, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class AggregationAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AggregationAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sm = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x).squeeze(2)\n",
    "        weights_att = self.sm(out).unsqueeze(2)\n",
    "        context = torch.sum(weights_att * x, 0)\n",
    "        return context\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth=1, heads=4, dim_head=16, mlp_dim=16, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "        self.aggregation = AggregationAttention(dim)\n",
    "    def forward(self, x):\n",
    "        # B F C 1\n",
    "        size = x.size()\n",
    "\t\t# --> B C F\n",
    "        x = x.view(*size[:2], -1).permute(0,2,1)\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        # --- B C F \n",
    "        x = self.aggregation(x.permute(1,0,2))\n",
    "        return x.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49202007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFC_V1_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape ,\n",
    "        number_class , \n",
    "        filter_num = 32,\n",
    "        filter_size = 5,\n",
    "        nb_conv_layers = 4,\n",
    "        dropout = 0.2,\n",
    "        hidden_dim = 32,\n",
    "        activation = \"ReLU\",\n",
    "        sa_div= 1,\n",
    "    ):\n",
    "        super(CFC_V1_Model, self).__init__()\n",
    "        \n",
    "        # PART 1 , Channel wise Feature Extraction\n",
    "        \n",
    "        layers_conv = []\n",
    "        for i in range(nb_conv_layers):\n",
    "        \n",
    "            if i == 0:\n",
    "                in_channel = input_shape[1]\n",
    "            else:\n",
    "                in_channel = filter_num\n",
    "            if i%2==0:\n",
    "                stride = 2\n",
    "            else:\n",
    "                stride = 2\n",
    "            layers_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channel, filter_num, (filter_size, 1),(stride,1)),#(2,1)\n",
    "                nn.ReLU(inplace=True),\n",
    "                #nn.BatchNorm2d(filter_num),\n",
    "            ))\n",
    "        \n",
    "        self.layers_conv = nn.ModuleList(layers_conv)\n",
    "\n",
    "        # PART2 , Cross Channel Fusion through Attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #self.sa = SelfAttention(filter_num, sa_div)\n",
    "        self.channel_aggregation = Transformer(dim=filter_num, depth=1, heads=4, dim_head=16, mlp_dim=filter_num, dropout = 0.)\n",
    "\n",
    "\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            filter_num,\n",
    "            hidden_dim,\n",
    "            2,\n",
    "            bidirectional=False,\n",
    "            dropout=0.15,\n",
    "        )\n",
    "        \n",
    "        self.temporal_aggregation = AggregationAttention(hidden_dim)\n",
    "\n",
    "#         # PART 3 , Prediction \n",
    "        self.prediction = nn.Linear(hidden_dim, number_class)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B F , L C  F =1 or F =filter * scale + 1\n",
    "\n",
    "        \n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)      \n",
    "        # B filter_num  L  C\n",
    "\n",
    "        batch, filter, length, channel = x.shape\n",
    "        # 每次进去的都是 B filter_num C 1\n",
    "        refined = torch.cat(\n",
    "            [self.channel_aggregation(torch.unsqueeze(x[:, :, t, :], dim=3)) for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # B F L\n",
    "        print(refined.shape)\n",
    "\n",
    "        x = refined.permute(2,0,1)\n",
    "        # L B F\n",
    "        x = self.dropout(x)\n",
    "        outputs, h = self.rnn(x)\n",
    "\t\n",
    "        # L B F\n",
    "        x = self.temporal_aggregation(outputs)\n",
    "        # B  F \n",
    "\n",
    "        y = self.prediction(x)    \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea1461e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0680,  0.1128, -0.1493,  0.1220,  0.1361, -0.1117],\n",
       "        [ 0.0524,  0.1450, -0.1422,  0.1411,  0.1466, -0.1003]],\n",
       "       dtype=torch.float64, grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batch = 2\n",
    "F_in  = 100\n",
    "Leng  = 128\n",
    "C_in = 6\n",
    "number_class = 6\n",
    "model = CFC_V1_Model((Batch,F_in,Leng, C_in),number_class,filter_num = 32).double()\n",
    "input = torch.rand(Batch,F_in,Leng, C_in).double()\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb03452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54888"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum([para.numel() for para in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73e223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1cc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c22347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8fb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07753f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d511d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb2073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005c3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9b258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f08cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a672132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874de806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc4a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbcd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f3eb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv1d(ni: int, no: int, ks: int = 1, stride: int = 1, padding: int = 0, bias: bool = False):\n",
    "    \"\"\"\n",
    "    Create and initialize a `nn.Conv1d` layer with spectral normalization.\n",
    "    \"\"\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias:\n",
    "        conv.bias.data.zero_()\n",
    "    # return spectral_norm(conv)\n",
    "    return conv\n",
    "\n",
    "class AggregationAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AggregationAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sm = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x).squeeze(2)\n",
    "        weights_att = self.sm(out).unsqueeze(2)\n",
    "        context = torch.sum(weights_att * x, 0)\n",
    "        return context\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels: int, div):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        if n_channels > 1:\n",
    "            self.query = conv1d(n_channels, n_channels//div)\n",
    "            self.key = conv1d(n_channels, n_channels//div)\n",
    "        else:\n",
    "            self.query = conv1d(n_channels, n_channels)\n",
    "            self.key = conv1d(n_channels, n_channels)\n",
    "        self.value = conv1d(n_channels, n_channels)\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "        self.ta = AggregationAttention(n_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Notation from https://arxiv.org/pdf/1805.08318.pdf\n",
    "        #print(\" 1 : \", x.shape)\n",
    "        size = x.size()\n",
    "        #print(\"size+\",size)\n",
    "        x = x.view(*size[:2], -1)\n",
    "        #print(\" 2 : \", x.shape)\n",
    "        #print(\"size-\",x.size())\n",
    "        f, g, h = self.query(x), self.key(x), self.value(x)\n",
    "        beta = F.softmax(torch.bmm(f.permute(0, 2, 1).contiguous(), g), dim=1)\n",
    "        o = self.gamma * torch.bmm(h, beta) + x\n",
    "        #print(\" 3 : \", o.shape)\n",
    "        ta = self.ta(o.permute(2,0,1))\n",
    "        #print(\"5 :\" ,ta.shape)\n",
    "        #print(\" 4 : \", o.view(*size).shape)\n",
    "        return o.view(*size).contiguous(),ta.unsqueeze(2)\n",
    "\n",
    "    \n",
    "class CFC_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape ,\n",
    "        number_class , \n",
    "        filter_num = 16,\n",
    "        filter_size = 5,\n",
    "        nb_conv_layers = 4,\n",
    "        dropout = 0.2,\n",
    "        activation = \"ReLU\",\n",
    "        sa_div= 1,\n",
    "    ):\n",
    "        super(CFC_Model, self).__init__()\n",
    "        \n",
    "        # PART 1 , Channel wise Feature Extraction\n",
    "        \n",
    "        layers_conv = []\n",
    "        for i in range(nb_conv_layers):\n",
    "        \n",
    "            if i == 0:\n",
    "                in_channel = 1\n",
    "            else:\n",
    "                in_channel = filter_num\n",
    "            if i%2==0:\n",
    "                stride = 2\n",
    "            else:\n",
    "                stride = 1\n",
    "            layers_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channel, filter_num, (filter_size, 1),(stride,1)),#(2,1)\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(filter_num),\n",
    "\n",
    "            ))\n",
    "        \n",
    "        self.layers_conv = nn.ModuleList(layers_conv)\n",
    "\n",
    "        # PART2 , Cross Channel Fusion through Attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sa = SelfAttention(filter_num, sa_div)\n",
    "        \n",
    "#         shape = self.get_the_shape(input_shape)\n",
    "\n",
    "#         # PART 3 , Prediction \n",
    "        \n",
    "#         self.activation = nn.ReLU() \n",
    "#         self.fc1 = nn.Linear(input_shape[2]*filter_num ,filter_num)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc2 = nn.Linear(shape[1]*filter_num ,filter_num)\n",
    "#         self.fc3 = nn.Linear(filter_num ,number_class)\n",
    "\n",
    "\n",
    "        \n",
    "    def get_the_shape(self, input_shape):\n",
    "        x = torch.rand(input_shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)    \n",
    "        atten_x = torch.cat(\n",
    "            [self.sa(torch.unsqueeze(x[:, :, t, :], dim=3))[0] for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        atten_x = atten_x.permute(0, 3, 1, 2)\n",
    "        return atten_x.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B L C\n",
    "        x = x.unsqueeze(1)\n",
    "        print(\"begin:\" , x.shape)\n",
    "        \n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)      \n",
    "\n",
    "\n",
    "        batch, filter, length, channel = x.shape\n",
    "\n",
    "        print(\"conv : \", x.shape)\n",
    "        # apply self-attention on each temporal dimension (along sensor and feature dimensions)\n",
    "#         refined = torch.cat(\n",
    "#             [self.sa(torch.unsqueeze(x[:, :, t, :], dim=3))[0] for t in range(x.shape[2])],\n",
    "#             dim=-1,\n",
    "#         )\n",
    "        refined = torch.cat(\n",
    "            [self.sa(torch.unsqueeze(x[:, :, t, :], dim=3))[1] for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        print(\"attn : \",refined.shape)\n",
    "       # print(refined.shape)\n",
    "\n",
    "#         x = refined.permute(0, 3, 1, 2)\n",
    "#         x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "#         print(\"reshape : \",x.shape)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         x = self.activation(self.fc1(x)) # B L C\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.activation(self.fc2(x)) # B L C\n",
    "#         y = self.fc3(x)    \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5cb43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv1d(ni: int, no: int, ks: int = 1, stride: int = 1, padding: int = 0, bias: bool = False):\n",
    "    \"\"\"\n",
    "    Create and initialize a `nn.Conv1d` layer with spectral normalization.\n",
    "    \"\"\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias:\n",
    "        conv.bias.data.zero_()\n",
    "    # return spectral_norm(conv)\n",
    "    return conv\n",
    "\n",
    "class AggregationAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AggregationAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sm = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x, \",x.shape)\n",
    "        out = self.fc(x).squeeze(2)\n",
    "        print(\"out, \",out.shape)\n",
    "        weights_att = self.sm(out).unsqueeze(2)\n",
    "        print(\"weights_att, \",weights_att.shape)\n",
    "        context = torch.sum(weights_att * x, 0)\n",
    "        print(\"context, \",context.shape)\n",
    "        return context\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels: int, div):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        if n_channels > 1:\n",
    "            self.query = conv1d(n_channels, n_channels//div)\n",
    "            self.key = conv1d(n_channels, n_channels//div)\n",
    "        else:\n",
    "            self.query = conv1d(n_channels, n_channels)\n",
    "            self.key = conv1d(n_channels, n_channels)\n",
    "        self.value = conv1d(n_channels, n_channels)\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "        self.ta = AggregationAttention(n_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Notation from https://arxiv.org/pdf/1805.08318.pdf\n",
    "        #print(\" 1 : \", x.shape)\n",
    "        size = x.size()\n",
    "        #print(\"size+\",size)\n",
    "        x = x.view(*size[:2], -1)\n",
    "        #print(\" 2 : \", x.shape)\n",
    "        #print(\"size-\",x.size())\n",
    "        f, g, h = self.query(x), self.key(x), self.value(x)\n",
    "        beta = F.softmax(torch.bmm(f.permute(0, 2, 1).contiguous(), g), dim=1)\n",
    "        o = self.gamma * torch.bmm(h, beta) + x\n",
    "        print(\" 3 : \", o.shape)\n",
    "        ta = self.ta(o.permute(2,0,1))\n",
    "        print(\"5 :\" ,ta.shape)\n",
    "        #print(\" 4 : \", o.view(*size).shape)\n",
    "        return o.view(*size).contiguous(),ta.unsqueeze(2)\n",
    "\n",
    "    \n",
    "class CFC_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape ,\n",
    "        number_class , \n",
    "        filter_num = 16,\n",
    "        filter_size = 5,\n",
    "        nb_conv_layers = 4,\n",
    "        dropout = 0.2,\n",
    "        hidden_dim = 32,\n",
    "        activation = \"ReLU\",\n",
    "        sa_div= 1,\n",
    "    ):\n",
    "        super(CFC_Model, self).__init__()\n",
    "        \n",
    "        # PART 1 , Channel wise Feature Extraction\n",
    "        \n",
    "        layers_conv = []\n",
    "        for i in range(nb_conv_layers):\n",
    "        \n",
    "            if i == 0:\n",
    "                in_channel = 1\n",
    "            else:\n",
    "                in_channel = filter_num\n",
    "            if i%2==0:\n",
    "                stride = 2\n",
    "            else:\n",
    "                stride = 1\n",
    "            layers_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channel, filter_num, (filter_size, 1),(stride,1)),#(2,1)\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(filter_num),\n",
    "\n",
    "            ))\n",
    "        \n",
    "        self.layers_conv = nn.ModuleList(layers_conv)\n",
    "\n",
    "        # PART2 , Cross Channel Fusion through Attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sa = SelfAttention(filter_num, sa_div)\n",
    "        self.rnn = nn.GRU(\n",
    "            filter_num,\n",
    "            hidden_dim,\n",
    "            2,\n",
    "            bidirectional=False,\n",
    "            dropout=0.15,\n",
    "        )\n",
    "        \n",
    "        self.ta = AggregationAttention(hidden_dim)\n",
    "#         shape = self.get_the_shape(input_shape)\n",
    "\n",
    "#         # PART 3 , Prediction \n",
    "        self.fc = nn.Linear(hidden_dim, number_class)\n",
    "#         self.activation = nn.ReLU() \n",
    "#         self.fc1 = nn.Linear(input_shape[2]*filter_num ,filter_num)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc2 = nn.Linear(shape[1]*filter_num ,filter_num)\n",
    "#         self.fc3 = nn.Linear(filter_num ,number_class)\n",
    "\n",
    "\n",
    "        \n",
    "    def get_the_shape(self, input_shape):\n",
    "        x = torch.rand(input_shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)    \n",
    "        atten_x = torch.cat(\n",
    "            [self.sa(torch.unsqueeze(x[:, :, t, :], dim=3))[0] for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        atten_x = atten_x.permute(0, 3, 1, 2)\n",
    "        return atten_x.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B L C\n",
    "        x = x.unsqueeze(1)\n",
    "        print(\"begin:\" , x.shape)\n",
    "        \n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)      \n",
    "\n",
    "\n",
    "        batch, filter, length, channel = x.shape\n",
    "\n",
    "        print(\"conv : \", x.shape)\n",
    "        # apply self-attention on each temporal dimension (along sensor and feature dimensions)\n",
    "#         refined = torch.cat(\n",
    "#             [self.sa(torch.unsqueeze(x[:, :, t, :], dim=3))[0] for t in range(x.shape[2])],\n",
    "#             dim=-1,\n",
    "#         )\n",
    "        refined = torch.cat(\n",
    "            [self.sa(torch.unsqueeze(x[:, :, t, :], dim=3))[1] for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        print(\"attn : \",refined.shape)\n",
    "       # print(refined.shape)\n",
    "\n",
    "        x = refined.permute(2,0,1)\n",
    "\n",
    "#         print(\"reshape : \",x.shape)\n",
    "        x = self.dropout(x)\n",
    "        outputs, h = self.rnn(x)\n",
    "        x = self.ta(outputs)\n",
    "        print(\"out : \",x.shape)\n",
    "#         x = self.activation(self.fc1(x)) # B L C\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.activation(self.fc2(x)) # B L C\n",
    "        y = self.fc(x)    \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111c777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "304be097",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin: torch.Size([2, 1, 256, 600])\n",
      "conv :  torch.Size([2, 16, 55, 600])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      " 3 :  torch.Size([2, 16, 600])\n",
      "x,  torch.Size([600, 2, 16])\n",
      "out,  torch.Size([600, 2])\n",
      "weights_att,  torch.Size([600, 2, 1])\n",
      "context,  torch.Size([2, 16])\n",
      "5 : torch.Size([2, 16])\n",
      "attn :  torch.Size([2, 16, 55])\n",
      "x,  torch.Size([55, 2, 32])\n",
      "out,  torch.Size([55, 2])\n",
      "weights_att,  torch.Size([55, 2, 1])\n",
      "context,  torch.Size([2, 32])\n",
      "out :  torch.Size([2, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1087, -0.0541, -0.0789, -0.1395, -0.0465,  0.0397],\n",
       "        [ 0.1144, -0.0556, -0.0767, -0.1404, -0.0492,  0.0441]],\n",
       "       dtype=torch.float64, grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = CFC_Model((2,256,600),6,filter_num = 16).double()\n",
    "input = torch.rand(2,256,600).double()\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "41e98100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16265"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum([para.numel() for para in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b2f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921912a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1733c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbacdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFC_New(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape ,\n",
    "        number_class , \n",
    "        filter_num = 16,\n",
    "        filter_size = 5,\n",
    "        nb_conv_layers = 4,\n",
    "        dropout = 0.2,\n",
    "        activation = \"ReLU\",\n",
    "        sa_div= 1,\n",
    "    ):\n",
    "        super(CFC_New, self).__init__()\n",
    "        \n",
    "        # PART 1 , Channel wise Feature Extraction\n",
    "        \n",
    "        layers_conv = []\n",
    "        for i in range(nb_conv_layers):\n",
    "        \n",
    "            if i == 0:\n",
    "                in_channel = 1\n",
    "            else:\n",
    "                in_channel = filter_num\n",
    "    \n",
    "            layers_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channel, filter_num, (filter_size, 1),(2,1)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(filter_num),\n",
    "\n",
    "            ))\n",
    "        \n",
    "        self.layers_conv = nn.ModuleList(layers_conv)\n",
    "\n",
    "        # PART2 , Cross Channel Fusion through Attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sa = SelfAttention(filter_num, sa_div)\n",
    "        \n",
    "        shape = self.get_the_shape(input_shape)\n",
    "\n",
    "        # PART 3 , Prediction \n",
    "        \n",
    "        self.activation = nn.ReLU() \n",
    "        self.fc1 = nn.Linear(input_shape[2]*filter_num ,filter_num)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc2 = nn.Linear(shape[1]*filter_num ,filter_num)\n",
    "        self.fc3 = nn.Linear(filter_num ,number_class)\n",
    "\n",
    "\n",
    "        \n",
    "    def get_the_shape(self, input_shape):\n",
    "        x = torch.rand(input_shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)    \n",
    "        atten_x = torch.cat(\n",
    "            [self.sa(torch.unsqueeze(x[:, :, t, :], dim=3)) for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "        atten_x = atten_x.permute(0, 3, 1, 2)\n",
    "        return atten_x.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B L C\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        for layer in self.layers_conv:\n",
    "            x = layer(x)      \n",
    "\n",
    "\n",
    "        batch, filter, length, channel = x.shape\n",
    "\n",
    "\n",
    "        # apply self-attention on each temporal dimension (along sensor and feature dimensions)\n",
    "        refined = torch.cat(\n",
    "            [self.sa(torch.unsqueeze(x[:, :, t, :], dim=3)) for t in range(x.shape[2])],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "\n",
    "       # print(refined.shape)\n",
    "\n",
    "        x = refined.permute(0, 3, 1, 2)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.activation(self.fc1(x)) # B L C\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc2(x)) # B L C\n",
    "        y = self.fc3(x)    \n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
