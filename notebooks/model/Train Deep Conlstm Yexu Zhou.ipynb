{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894e2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from experiment import Exp\n",
    "\n",
    "from dataloaders import data_set,data_dict\n",
    "import torch\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc56bc7",
   "metadata": {},
   "source": [
    "# 参数设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5bcbc",
   "metadata": {},
   "source": [
    "# 训练参数 \n",
    "除了路径 其他不要变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86004ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "args = dotdict()   \n",
    "# TODO change the path as relative path\n",
    "args.to_save_path     = r\"E:\\TECO_Works\\Conference\\ISWC2022\\Run_logs\"              \n",
    "args.freq_save_path   = r\"E:\\TECO_Works\\Conference\\ISWC2022\\Freq_data\"\n",
    "args.window_save_path = r\"E:\\TECO_Works\\Conference\\ISWC2022\\Sliding_window\"\n",
    "args.root_path        = r\"E:\\datasets\"\n",
    "\n",
    "\n",
    "args.drop_transition  = False\n",
    "args.datanorm_type    = \"standardization\" # None ,\"standardization\", \"minmax\"\n",
    "\n",
    "\n",
    "args.batch_size       = 256                                                     \n",
    "args.shuffle          = True\n",
    "args.drop_last        = False\n",
    "args.train_vali_quote = 0.90                                           \n",
    "\n",
    "\n",
    "# training setting \n",
    "args.train_epochs            = 50\n",
    "\n",
    "args.learning_rate           = 0.001  \n",
    "args.learning_rate_patience  = 5\n",
    "args.learning_rate_factor    = 0.1\n",
    "\n",
    "\n",
    "args.early_stop_patience     = 15\n",
    "\n",
    "args.use_gpu                 = True if torch.cuda.is_available() else False\n",
    "args.gpu                     = 0\n",
    "args.use_multi_gpu           = False\n",
    "\n",
    "args.optimizer               = \"Adam\"\n",
    "args.criterion               = \"CrossEntropy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282cbcb",
   "metadata": {},
   "source": [
    "## 数据参数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cd147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.seed                             = 1\n",
    "\n",
    "\n",
    "args.data_name                        =  \"skodar\"\n",
    "\n",
    "args.wavelet_filtering                = False\n",
    "args.wavelet_filtering_regularization = False\n",
    "args.wavelet_filtering_finetuning     = False\n",
    "args.wavelet_filtering_finetuning_percent = 0.5\n",
    "args.wavelet_filtering_learnable      = False\n",
    "args.wavelet_filtering_layernorm      = False\n",
    "\n",
    "args.regulatization_tradeoff          = 0\n",
    "args.number_wavelet_filtering         = 6\n",
    "\n",
    "\n",
    "args.difference       = False \n",
    "args.filtering        =  False\n",
    "args.magnitude        =  False\n",
    "args.weighted_sampler = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args.pos_select       = None\n",
    "args.sensor_select    = None\n",
    "\n",
    "\n",
    "args.representation_type = \"time\"\n",
    "args.exp_mode            = \"LOCV\"\n",
    "if args.data_name      ==  \"skodar\":\n",
    "    args.exp_mode            = \"SOCV\"\n",
    "config_file = open('../../configs/data.yaml', mode='r')\n",
    "data_config = yaml.load(config_file, Loader=yaml.FullLoader)\n",
    "config = data_config[args.data_name]\n",
    "\n",
    "args.root_path       = os.path.join(args.root_path,config[\"filename\"])\n",
    "args.sampling_freq   = config[\"sampling_freq\"]\n",
    "args.num_classes     =  config[\"num_classes\"]\n",
    "window_seconds       = config[\"window_seconds\"]\n",
    "args.windowsize      =   int(window_seconds * args.sampling_freq) \n",
    "args.input_length    =  args.windowsize\n",
    "# input information\n",
    "args.c_in            = config[\"num_channels\"]\n",
    "\n",
    "\n",
    "if args.difference:\n",
    "    args.c_in = args.c_in*2\n",
    "\n",
    "if args.wavelet_filtering :\n",
    "    \n",
    "    if args.windowsize%2==1:\n",
    "        N_ds = int(torch.log2(torch.tensor(args.windowsize-1)).floor()) - 2\n",
    "    else:\n",
    "        N_ds = int(torch.log2(torch.tensor(args.windowsize)).floor()) - 2\n",
    "\n",
    "    args.f_in            =  args.number_wavelet_filtering*N_ds+1\n",
    "else:\n",
    "    args.f_in            =  1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d435a4c",
   "metadata": {},
   "source": [
    "## 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2f4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.filter_scaling_factor = 0.25\n",
    "args.model_type              = \"deepconvlstm\"#\"deepconvlstm\"#\"sahar\" #\"deepconvlstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbca73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_type              = \"tinyhar\"#\"deepconvlstm\"#\"sahar\" #\"deepconvlstm\"\n",
    "\n",
    "args.cross_channel_interaction_type = \"attn\"\n",
    "args.cross_channel_aggregation_type = \"FC\"\n",
    "args.temporal_info_interaction_type = \"attn\"\n",
    "args.temporal_info_aggregation_type = \"naive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1595a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_channel_interaction_type = \"attn\",    # attn  transformer  identity\n",
    "# cross_channel_aggregation_type = \"filter\",  # filter  naive  FC\n",
    "# temporal_info_interaction_type = \"gru\",     # gru  lstm  attn  transformer  identity\n",
    "# temporal_info_aggregation_type = \"FC\",      # naive  filter  FC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada66dd",
   "metadata": {},
   "source": [
    "# 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3f2fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "Build the TinyHAR model!\n",
      "Done!\n",
      "Parameter : 43941\n",
      "Set the seed as :  1\n"
     ]
    }
   ],
   "source": [
    "# 如果我们设置为 \n",
    "args.wavelet_filtering_learnable      = False\n",
    "exp = Exp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2185d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果我们设置为 \n",
    "# args.wavelet_filtering_learnable      = True\n",
    "# exp = Exp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "753ebb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_builder(\n",
       "  (model): TinyHAR_Model(\n",
       "    (layers_conv): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(1, 8, kernel_size=(5, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(8, 16, kernel_size=(5, 1), stride=(2, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(16, 24, kernel_size=(5, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(24, 32, kernel_size=(5, 1), stride=(2, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (channel_interaction): SelfAttention_interaction(\n",
       "      (query): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (key): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (value): Linear(in_features=32, out_features=32, bias=False)\n",
       "    )\n",
       "    (channel_fusion): FC(\n",
       "      (fc): Linear(in_features=960, out_features=32, bias=True)\n",
       "    )\n",
       "    (activation): ReLU()\n",
       "    (temporal_interaction): SelfAttention_interaction(\n",
       "      (query): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (key): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (value): Linear(in_features=32, out_features=32, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (temporal_fusion): NaiveWeighted_Aggregation(\n",
       "      (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (sm): Softmax(dim=1)\n",
       "    )\n",
       "    (prediction): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8aed663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----------------------- load all the data -------------------\n",
      "-----------------------Sliding file are generated -----------------------\n",
      "================ SOCV Mode ====================\n",
      "================ 5 CV ======================\n",
      "================ the 0 th CV Experiment ================ \n",
      "Overlapping random Experiment : The 1 Part as the test\n",
      "[-] Target sampling weights:  [0.00277778 0.00238095 0.002457   0.00337838 0.00540541 0.00558659\n",
      " 0.00306748 0.00278552 0.00234192 0.00408163]\n",
      "Train data number :  3204\n",
      "The number of classes is :  10\n",
      "The input_length  is :  84\n",
      "The channel_in is :  30\n",
      "Validation data number :  356\n",
      "Test data number :  890\n",
      "================ Build the model ================ \n",
      "Build the TinyHAR model!\n",
      "Epoch: 1 cost time: 3.428325653076172\n",
      "VALI: Epoch: 1, Steps: 13 | Train Loss: 1.6274259  Vali Loss: 1.8293062 Vali Accuracy: 0.4578652  Vali weighted F1: 0.4056690  Vali macro F1 0.3660537 \n",
      "Validation loss decreased (inf --> 1.829306).  Saving model ...\n",
      "Epoch: 2 cost time: 1.9744446277618408\n",
      "VALI: Epoch: 2, Steps: 13 | Train Loss: 0.7847006  Vali Loss: 0.7610017 Vali Accuracy: 0.7415730  Vali weighted F1: 0.7303805  Vali macro F1 0.6908886 \n",
      "new best score!!!!\n",
      "Validation loss decreased (1.829306 --> 0.761002).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 3 cost time: 1.965442180633545\n",
      "VALI: Epoch: 3, Steps: 13 | Train Loss: 0.5142376  Vali Loss: 0.5266843 Vali Accuracy: 0.7668539  Vali weighted F1: 0.7527704  Vali macro F1 0.7136869 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.761002 --> 0.526684).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 4 cost time: 1.9674427509307861\n",
      "VALI: Epoch: 4, Steps: 13 | Train Loss: 0.3802576  Vali Loss: 0.4236304 Vali Accuracy: 0.8539326  Vali weighted F1: 0.8529952  Vali macro F1 0.8463060 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.526684 --> 0.423630).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 5 cost time: 1.9624412059783936\n",
      "VALI: Epoch: 5, Steps: 13 | Train Loss: 0.2978362  Vali Loss: 0.3296385 Vali Accuracy: 0.8820225  Vali weighted F1: 0.8786901  Vali macro F1 0.8514130 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.423630 --> 0.329639).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 6 cost time: 1.9704439640045166\n",
      "VALI: Epoch: 6, Steps: 13 | Train Loss: 0.2341540  Vali Loss: 0.2325954 Vali Accuracy: 0.9241573  Vali weighted F1: 0.9232547  Vali macro F1 0.9087496 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.329639 --> 0.232595).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 7 cost time: 1.9604411125183105\n",
      "VALI: Epoch: 7, Steps: 13 | Train Loss: 0.1874299  Vali Loss: 0.2154577 Vali Accuracy: 0.9073034  Vali weighted F1: 0.9060486  Vali macro F1 0.8913553 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.232595 --> 0.215458).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 8 cost time: 1.9734439849853516\n",
      "VALI: Epoch: 8, Steps: 13 | Train Loss: 0.1764659  Vali Loss: 0.1590204 Vali Accuracy: 0.9410112  Vali weighted F1: 0.9404222  Vali macro F1 0.9196245 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.215458 --> 0.159020).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 9 cost time: 1.9654409885406494\n",
      "VALI: Epoch: 9, Steps: 13 | Train Loss: 0.1582239  Vali Loss: 0.1819004 Vali Accuracy: 0.9073034  Vali weighted F1: 0.9021394  Vali macro F1 0.8646129 \n",
      "EarlyStopping counter: 1 out of 15\n",
      "Learning rate adjusting counter: 1 out of 5\n",
      "Epoch: 10 cost time: 1.9754445552825928\n",
      "VALI: Epoch: 10, Steps: 13 | Train Loss: 0.1247869  Vali Loss: 0.1619953 Vali Accuracy: 0.9213483  Vali weighted F1: 0.9207133  Vali macro F1 0.9024319 \n",
      "EarlyStopping counter: 2 out of 15\n",
      "Learning rate adjusting counter: 2 out of 5\n",
      "Epoch: 11 cost time: 1.9634425640106201\n",
      "VALI: Epoch: 11, Steps: 13 | Train Loss: 0.0947522  Vali Loss: 0.1324909 Vali Accuracy: 0.9438202  Vali weighted F1: 0.9437773  Vali macro F1 0.9368185 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.159020 --> 0.132491).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 12 cost time: 1.988447904586792\n",
      "VALI: Epoch: 12, Steps: 13 | Train Loss: 0.0775676  Vali Loss: 0.1185465 Vali Accuracy: 0.9522472  Vali weighted F1: 0.9521678  Vali macro F1 0.9502095 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.132491 --> 0.118547).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 13 cost time: 1.965442419052124\n",
      "VALI: Epoch: 13, Steps: 13 | Train Loss: 0.0688130  Vali Loss: 0.1264801 Vali Accuracy: 0.9522472  Vali weighted F1: 0.9522221  Vali macro F1 0.9530702 \n",
      "EarlyStopping counter: 1 out of 15\n",
      "Learning rate adjusting counter: 1 out of 5\n",
      "Epoch: 14 cost time: 1.9714438915252686\n",
      "VALI: Epoch: 14, Steps: 13 | Train Loss: 0.0587001  Vali Loss: 0.1078862 Vali Accuracy: 0.9578652  Vali weighted F1: 0.9576752  Vali macro F1 0.9549316 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.118547 --> 0.107886).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 15 cost time: 1.9864473342895508\n",
      "VALI: Epoch: 15, Steps: 13 | Train Loss: 0.0533759  Vali Loss: 0.1186451 Vali Accuracy: 0.9494382  Vali weighted F1: 0.9495869  Vali macro F1 0.9449738 \n",
      "EarlyStopping counter: 1 out of 15\n",
      "Learning rate adjusting counter: 1 out of 5\n",
      "Epoch: 16 cost time: 1.9674427509307861\n",
      "VALI: Epoch: 16, Steps: 13 | Train Loss: 0.0449861  Vali Loss: 0.1109128 Vali Accuracy: 0.9522472  Vali weighted F1: 0.9519800  Vali macro F1 0.9381326 \n",
      "EarlyStopping counter: 2 out of 15\n",
      "Learning rate adjusting counter: 2 out of 5\n",
      "Epoch: 17 cost time: 2.0254576206207275\n",
      "VALI: Epoch: 17, Steps: 13 | Train Loss: 0.0431874  Vali Loss: 0.1019942 Vali Accuracy: 0.9494382  Vali weighted F1: 0.9492063  Vali macro F1 0.9359618 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.107886 --> 0.101994).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 18 cost time: 1.9824459552764893\n",
      "VALI: Epoch: 18, Steps: 13 | Train Loss: 0.0360225  Vali Loss: 0.0906938 Vali Accuracy: 0.9494382  Vali weighted F1: 0.9491674  Vali macro F1 0.9360262 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.101994 --> 0.090694).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 19 cost time: 2.0294570922851562\n",
      "VALI: Epoch: 19, Steps: 13 | Train Loss: 0.0306296  Vali Loss: 0.0818057 Vali Accuracy: 0.9691011  Vali weighted F1: 0.9691348  Vali macro F1 0.9688477 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.090694 --> 0.081806).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 20 cost time: 2.042462110519409\n",
      "VALI: Epoch: 20, Steps: 13 | Train Loss: 0.0310532  Vali Loss: 0.0957912 Vali Accuracy: 0.9578652  Vali weighted F1: 0.9578108  Vali macro F1 0.9493357 \n",
      "EarlyStopping counter: 1 out of 15\n",
      "Learning rate adjusting counter: 1 out of 5\n",
      "Epoch: 21 cost time: 1.9754440784454346\n",
      "VALI: Epoch: 21, Steps: 13 | Train Loss: 0.0286125  Vali Loss: 0.0795530 Vali Accuracy: 0.9691011  Vali weighted F1: 0.9691396  Vali macro F1 0.9652913 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.081806 --> 0.079553).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 22 cost time: 2.0604684352874756\n",
      "VALI: Epoch: 22, Steps: 13 | Train Loss: 0.0239048  Vali Loss: 0.0985107 Vali Accuracy: 0.9578652  Vali weighted F1: 0.9578595  Vali macro F1 0.9420049 \n",
      "EarlyStopping counter: 1 out of 15\n",
      "Learning rate adjusting counter: 1 out of 5\n",
      "Epoch: 23 cost time: 2.021458625793457\n",
      "VALI: Epoch: 23, Steps: 13 | Train Loss: 0.0218838  Vali Loss: 0.0817541 Vali Accuracy: 0.9747191  Vali weighted F1: 0.9747640  Vali macro F1 0.9712332 \n",
      "EarlyStopping counter: 2 out of 15\n",
      "Learning rate adjusting counter: 2 out of 5\n",
      "Epoch: 24 cost time: 2.0464606285095215\n",
      "VALI: Epoch: 24, Steps: 13 | Train Loss: 0.0194217  Vali Loss: 0.0909466 Vali Accuracy: 0.9691011  Vali weighted F1: 0.9692585  Vali macro F1 0.9634539 \n",
      "EarlyStopping counter: 3 out of 15\n",
      "Learning rate adjusting counter: 3 out of 5\n",
      "Epoch: 25 cost time: 2.024456262588501\n",
      "VALI: Epoch: 25, Steps: 13 | Train Loss: 0.0243226  Vali Loss: 0.0771364 Vali Accuracy: 0.9606742  Vali weighted F1: 0.9606231  Vali macro F1 0.9512582 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.079553 --> 0.077136).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 26 cost time: 2.043459177017212\n",
      "VALI: Epoch: 26, Steps: 13 | Train Loss: 0.0221168  Vali Loss: 0.1036119 Vali Accuracy: 0.9606742  Vali weighted F1: 0.9606430  Vali macro F1 0.9445055 \n",
      "EarlyStopping counter: 1 out of 15\n",
      "Learning rate adjusting counter: 1 out of 5\n",
      "Epoch: 27 cost time: 2.0184547901153564\n",
      "VALI: Epoch: 27, Steps: 13 | Train Loss: 0.0300580  Vali Loss: 0.1507420 Vali Accuracy: 0.9494382  Vali weighted F1: 0.9497067  Vali macro F1 0.9275520 \n",
      "EarlyStopping counter: 2 out of 15\n",
      "Learning rate adjusting counter: 2 out of 5\n",
      "Epoch: 28 cost time: 1.9964494705200195\n",
      "VALI: Epoch: 28, Steps: 13 | Train Loss: 0.0191205  Vali Loss: 0.0850934 Vali Accuracy: 0.9634831  Vali weighted F1: 0.9636754  Vali macro F1 0.9586981 \n",
      "EarlyStopping counter: 3 out of 15\n",
      "Learning rate adjusting counter: 3 out of 5\n",
      "Epoch: 29 cost time: 1.9688949584960938\n",
      "VALI: Epoch: 29, Steps: 13 | Train Loss: 0.0190734  Vali Loss: 0.0748861 Vali Accuracy: 0.9719101  Vali weighted F1: 0.9719392  Vali macro F1 0.9719578 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.077136 --> 0.074886).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 30 cost time: 1.9714434146881104\n",
      "VALI: Epoch: 30, Steps: 13 | Train Loss: 0.0179865  Vali Loss: 0.0686249 Vali Accuracy: 0.9747191  Vali weighted F1: 0.9748061  Vali macro F1 0.9691041 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.074886 --> 0.068625).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 31 cost time: 2.03840708732605\n",
      "VALI: Epoch: 31, Steps: 13 | Train Loss: 0.0151567  Vali Loss: 0.0962897 Vali Accuracy: 0.9578652  Vali weighted F1: 0.9576255  Vali macro F1 0.9465201 \n",
      "EarlyStopping counter: 1 out of 15\n",
      "Learning rate adjusting counter: 1 out of 5\n",
      "Epoch: 32 cost time: 2.042459726333618\n",
      "VALI: Epoch: 32, Steps: 13 | Train Loss: 0.0205714  Vali Loss: 0.0701323 Vali Accuracy: 0.9719101  Vali weighted F1: 0.9719138  Vali macro F1 0.9663687 \n",
      "EarlyStopping counter: 2 out of 15\n",
      "Learning rate adjusting counter: 2 out of 5\n",
      "Epoch: 33 cost time: 1.9764454364776611\n",
      "VALI: Epoch: 33, Steps: 13 | Train Loss: 0.0193460  Vali Loss: 0.0803505 Vali Accuracy: 0.9662921  Vali weighted F1: 0.9662545  Vali macro F1 0.9629842 \n",
      "EarlyStopping counter: 3 out of 15\n",
      "Learning rate adjusting counter: 3 out of 5\n",
      "Epoch: 34 cost time: 1.985438346862793\n",
      "VALI: Epoch: 34, Steps: 13 | Train Loss: 0.0181463  Vali Loss: 0.0770648 Vali Accuracy: 0.9803371  Vali weighted F1: 0.9802979  Vali macro F1 0.9790282 \n",
      "EarlyStopping counter: 4 out of 15\n",
      "Learning rate adjusting counter: 4 out of 5\n",
      "Epoch: 35 cost time: 1.9754526615142822\n",
      "VALI: Epoch: 35, Steps: 13 | Train Loss: 0.0141510  Vali Loss: 0.0784613 Vali Accuracy: 0.9606742  Vali weighted F1: 0.9607064  Vali macro F1 0.9478278 \n",
      "EarlyStopping counter: 5 out of 15\n",
      "Learning rate adjusting counter: 5 out of 5\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 36 cost time: 1.9854469299316406\n",
      "VALI: Epoch: 36, Steps: 13 | Train Loss: 0.0116451  Vali Loss: 0.0577100 Vali Accuracy: 0.9719101  Vali weighted F1: 0.9719103  Vali macro F1 0.9692720 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.068625 --> 0.057710).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 37 cost time: 1.9954407215118408\n",
      "VALI: Epoch: 37, Steps: 13 | Train Loss: 0.0119819  Vali Loss: 0.0523255 Vali Accuracy: 0.9803371  Vali weighted F1: 0.9802664  Vali macro F1 0.9828817 \n",
      "new best score!!!!\n",
      "Validation loss decreased (0.057710 --> 0.052325).  Saving model ...\n",
      "new best score!!!!\n",
      "Epoch: 38 cost time: 1.9784371852874756\n",
      "VALI: Epoch: 38, Steps: 13 | Train Loss: 0.0097327  Vali Loss: 0.0598164 Vali Accuracy: 0.9775281  Vali weighted F1: 0.9775248  Vali macro F1 0.9744003 \n",
      "EarlyStopping counter: 1 out of 15\n",
      "Learning rate adjusting counter: 1 out of 5\n",
      "Epoch: 39 cost time: 1.981454849243164\n",
      "VALI: Epoch: 39, Steps: 13 | Train Loss: 0.0077225  Vali Loss: 0.0575719 Vali Accuracy: 0.9775281  Vali weighted F1: 0.9774999  Vali macro F1 0.9740442 \n",
      "EarlyStopping counter: 2 out of 15\n",
      "Learning rate adjusting counter: 2 out of 5\n",
      "Epoch: 40 cost time: 1.9894402027130127\n",
      "VALI: Epoch: 40, Steps: 13 | Train Loss: 0.0076064  Vali Loss: 0.0550121 Vali Accuracy: 0.9775281  Vali weighted F1: 0.9774999  Vali macro F1 0.9740442 \n",
      "EarlyStopping counter: 3 out of 15\n",
      "Learning rate adjusting counter: 3 out of 5\n",
      "Epoch: 41 cost time: 1.9814465045928955\n",
      "VALI: Epoch: 41, Steps: 13 | Train Loss: 0.0071534  Vali Loss: 0.0574089 Vali Accuracy: 0.9775281  Vali weighted F1: 0.9774999  Vali macro F1 0.9740442 \n",
      "EarlyStopping counter: 4 out of 15\n",
      "Learning rate adjusting counter: 4 out of 5\n",
      "Epoch: 42 cost time: 1.980445384979248\n",
      "VALI: Epoch: 42, Steps: 13 | Train Loss: 0.0073899  Vali Loss: 0.0559504 Vali Accuracy: 0.9775281  Vali weighted F1: 0.9774865  Vali macro F1 0.9802839 \n",
      "EarlyStopping counter: 5 out of 15\n",
      "Learning rate adjusting counter: 5 out of 5\n",
      "Updating learning rate to 1e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-21de09e58c74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\TECO_Works\\Conference\\ISWC2022\\I2S0W2C2_CFC\\experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                         \u001b[0mmodel_optim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d97ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6c81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba8aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb5ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d066180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a596b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fb08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04508e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ae571",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_dict[args.data_name](args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = dataset.train_slidingwindows[0][1]\n",
    "end_index   = dataset.train_slidingwindows[0][2]\n",
    "sample_x_1    = dataset.data_x.iloc[start_index:end_index, 1:-1].values\n",
    "\n",
    "start_index = dataset.train_slidingwindows[100][1]\n",
    "end_index   = dataset.train_slidingwindows[100][2]\n",
    "sample_x_2    = dataset.data_x.iloc[start_index:end_index, 1:-1].values\n",
    "\n",
    "temp_1 = np.expand_dims(sample_x_1,0)\n",
    "temp_2 = np.expand_dims(sample_x_2,0)\n",
    "combined_x = np.concatenate([temp_1,temp_2],axis=0)\n",
    "combined_x = np.expand_dims(combined_x,1)\n",
    "combined_x = torch.tensor(combined_x).double().to(exp.device)\n",
    "print(combined_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = exp.model(combined_x)\n",
    "out = out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = exp.model.wave_conv.wavelet_conv.weight.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,p in exp.model.named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index1 = 0\n",
    "index2 = 0\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(out[0,0,:,index2])\n",
    "plt.plot(out[0,1,:,index2])\n",
    "plt.plot(out[0,2,:,index2])\n",
    "plt.plot(out[0,3,:,index2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae8916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe11eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果我们设置为 \n",
    "# args.wavelet_filtering_learnable      = True\n",
    "# exp = Exp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15a68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beec6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f99481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8700989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b222186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae27337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506b3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5dc27e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
